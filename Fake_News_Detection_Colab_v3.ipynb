{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a53d190",
      "metadata": {},
      "source": [
        "# Fake News Detection \u2014 BERT + CNN + Metadata (Hybrid Ensemble)\n",
        "\n",
        "This Google Colab notebook trains a **BERT + metadata** model and a **CNN** text model, then builds a **hybrid ensemble**. Dataset: **clmentbisaillon/fake-and-real-news-dataset** (Fake.csv + True.csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import zipfile, os\n",
        "from IPython.display import display\n",
        "import pandas as pd\n\n",
        "print('\ud83d\udce6 Upload your dataset ZIP file (e.g., archive.zip) in the left sidebar Files section.')\n",
        "zip_path = '/content/archive.zip'\n",
        "extract_path = '/content/fake_news_dataset'\n\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print('\u2705 Dataset extracted successfully!')\n",
        "else:\n",
        "    print('\u26a0\ufe0f Please upload archive.zip first.')\n\n",
        "# Check extracted files\n",
        "if os.path.exists(extract_path):\n",
        "    print('\ud83d\udcc2 Extracted files:', os.listdir(extract_path))\n",
        "    fake_path = os.path.join(extract_path, 'Fake.csv')\n",
        "    true_path = os.path.join(extract_path, 'True.csv')\n",
        "    \n",
        "    # Quick data preview\n",
        "    try:\n",
        "        fake_df = pd.read_csv(fake_path).head()\n",
        "        true_df = pd.read_csv(true_path).head()\n",
        "        print('\\n\ud83d\udcf0 Preview of Fake.csv:')\n",
        "        display(fake_df)\n",
        "        print('\\n\ud83d\udcf0 Preview of True.csv:')\n",
        "        display(true_df)\n",
        "    except Exception as e:\n",
        "        print('\u26a0\ufe0f Error reading CSV files:', e)\n",
        "else:\n",
        "    print('\u26a0\ufe0f Dataset folder not found after extraction.')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "print('\ud83d\udd0d Checking dataset availability before training...')\n",
        "if os.path.exists(fake_path) and os.path.exists(true_path):\n",
        "    print('\u2705 Both Fake.csv and True.csv found! Ready for preprocessing and training.')\n",
        "else:\n",
        "    raise FileNotFoundError('\u274c Dataset files not found! Please ensure archive.zip was uploaded and extracted correctly.')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e52a0e96",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "!pip install -q transformers torch torchvision torchaudio --upgrade\n",
        "!pip install -q tensorflow keras scikit-learn tqdm nltk gensim kaggle\n",
        "print('Packages installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c9abd8a",
      "metadata": {},
      "source": [
        "## Kaggle dataset\n",
        "\n",
        "Upload your `kaggle.json` (API token) when prompted, so the notebook can download the dataset automatically.\n",
        "\n",
        "Steps:\n",
        "1. Go to https://www.kaggle.com/ -> Account -> Create API token -> download `kaggle.json`.\n",
        "2. Upload the file in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca537ada",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "print('Please upload your kaggle.json (Kaggle API token). If you already uploaded, ignore this step.')\n",
        "uploaded = files.upload()  # choose kaggle.json here\n",
        "for fn in uploaded.keys():\n",
        "    print('Uploaded file:', fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a25d466f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Move kaggle.json to ~/.kaggle and set permissions\n",
        "import os, shutil\n",
        "if os.path.exists('kaggle.json'):\n",
        "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "    shutil.move('kaggle.json', '/root/.kaggle/kaggle.json')\n",
        "    os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "    print('kaggle.json moved to /root/.kaggle/kaggle.json')\n",
        "else:\n",
        "    print('kaggle.json not found; if you already placed it manually, that is fine.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a17f1311",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset from Kaggle (clmentbisaillon/fake-and-real-news-dataset)\n",
        "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset -q\n",
        "!unzip -o -q fake-and-real-news-dataset.zip -d dataset\n",
        "print('Dataset downloaded and extracted to ./dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dbddf53",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and NLTK download\n",
        "import pandas as pd, numpy as np, torch, re, os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print('Imports ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f054cda7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Fake.csv and True.csv and combine into a single dataframe\n",
        "fake_df = pd.read_csv('dataset/Fake.csv')\n",
        "true_df = pd.read_csv('dataset/True.csv')\n",
        "\n",
        "fake_df['label'] = 'FAKE'\n",
        "true_df['label'] = 'REAL'\n",
        "\n",
        "df = pd.concat([fake_df, true_df], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print('Total samples:', len(df))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc82ee9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess: combine title + text -> content; create example credibility metadata\n",
        "df['content'] = df['title'].fillna('') + ' ' + df['text'].fillna('')\n",
        "df = df.dropna(subset=['content']).drop_duplicates(subset=['content']).reset_index(drop=True)\n",
        "\n",
        "def pseudo_source_from_title(title):\n",
        "    if isinstance(title, str):\n",
        "        t = title.lower()\n",
        "        if 'nytimes' in t or 'nyt' in t: return 'nytimes.com'\n",
        "        if 'bbc' in t: return 'bbc.com'\n",
        "        if 'cnn' in t: return 'cnn.com'\n",
        "        if 'fox' in t: return 'foxnews.com'\n",
        "    return 'unknown'\n",
        "\n",
        "df['source'] = df['title'].apply(pseudo_source_from_title)\n",
        "credibility_scores = {'bbc.com': 0.95, 'nytimes.com': 0.9, 'cnn.com': 0.85, 'foxnews.com': 0.6, 'unknown': 0.5}\n",
        "df['credibility'] = df['source'].map(credibility_scores).fillna(0.5)\n",
        "\n",
        "df['label_enc'] = df['label'].map({'REAL': 0, 'FAKE': 1})\n",
        "\n",
        "print('Prepared dataframe with credibility metadata')\n",
        "df[['title','source','credibility','label']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837a9716",
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT tokenizer encodings (using transformers)\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_texts, test_texts, y_train, y_test, train_meta, test_meta = train_test_split(\n",
        "    df['content'].values, df['label_enc'].values, df['credibility'].values, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels, metadata):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.metadata = metadata\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k,v in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
        "        item['metadata'] = torch.tensor(self.metadata[idx], dtype=torch.float)\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, y_train, train_meta)\n",
        "test_dataset = NewsDataset(test_encodings, y_test, test_meta)\n",
        "print('Datasets ready', len(train_dataset), len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b4a4a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT model with metadata head\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertWithMetadata(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertWithMetadata, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc1 = nn.Linear(768 + 1, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "    def forward(self, input_ids, attention_mask, metadata):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = outputs.pooler_output\n",
        "        combined = torch.cat((pooled, metadata.unsqueeze(1)), dim=1)\n",
        "        x = self.dropout(combined)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_bert = BertWithMetadata().to(device)\n",
        "print('BERT model ready on', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a08e15e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop for BERT (small demo: 2 epochs)\n",
        "from transformers import AdamW\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model_bert.parameters(), lr=2e-5)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "model_bert.train()\n",
        "\n",
        "for epoch in range(2):\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        metadata = batch['metadata'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model_bert(input_ids, attention_mask, metadata)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1} average loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "torch.save(model_bert.state_dict(), 'bert_metadata_model.pt')\n",
        "print('Saved bert_metadata_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0a46678",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN model (text-only) using Keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "def clean_text_simple(text):\n",
        "    text = re.sub(r'[^a-zA-Z ]', '', str(text))\n",
        "    text = text.lower()\n",
        "    return ' '.join([w for w in text.split() if w not in stop])\n",
        "\n",
        "df['clean_text'] = df['content'].apply(clean_text_simple)\n",
        "\n",
        "MAX_WORDS = 5000\n",
        "MAX_LEN = 200\n",
        "tokenizer_cnn = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer_cnn.fit_on_texts(df['clean_text'])\n",
        "X = tokenizer_cnn.texts_to_sequences(df['clean_text'])\n",
        "X = pad_sequences(X, maxlen=MAX_LEN)\n",
        "y_all = np.array(df['label_enc'])\n",
        "\n",
        "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "model_cnn = Sequential([\n",
        "    Embedding(MAX_WORDS, 100, input_length=MAX_LEN),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model_cnn.fit(X_train_cnn, y_train_cnn, epochs=5, batch_size=64, validation_split=0.2)\n",
        "model_cnn.save('cnn_fake_news_model.h5')\n",
        "print('Saved cnn_fake_news_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e207ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate BERT on test set and CNN, then ensemble\n",
        "model_bert.eval()\n",
        "bert_probs = []\n",
        "with torch.no_grad():\n",
        "    for batch in DataLoader(test_dataset, batch_size=8):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        metadata = batch['metadata'].to(device)\n",
        "        outputs = model_bert(input_ids, attention_mask, metadata)\n",
        "        probs = torch.softmax(outputs, dim=1)[:,1]\n",
        "        bert_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "cnn_probs = model_cnn.predict(X_test_cnn).flatten()\n",
        "\n",
        "n = min(len(bert_probs), len(cnn_probs))\n",
        "bert_probs = np.array(bert_probs[:n])\n",
        "cnn_probs = np.array(cnn_probs[:n])\n",
        "labels_for_eval = y_test_cnn[:n]\n",
        "\n",
        "final_probs = 0.7*bert_probs + 0.3*cnn_probs\n",
        "final_preds = (final_probs > 0.5).astype(int)\n",
        "\n",
        "print('BERT (sample) Accuracy:', accuracy_score(labels_for_eval, (bert_probs>0.5).astype(int)))\n",
        "print('CNN (sample) Accuracy:', accuracy_score(labels_for_eval, (cnn_probs>0.5).astype(int)))\n",
        "print('Ensemble Accuracy:', accuracy_score(labels_for_eval, final_preds))\n",
        "print('\\nClassification report (Ensemble):\\n', classification_report(labels_for_eval, final_preds))"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}